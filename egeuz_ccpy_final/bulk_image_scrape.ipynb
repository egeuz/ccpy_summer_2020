{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import re\n",
    "import time"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Form Subreddit Index\n",
    "First, let's go through a comprehensive list of subreddits, and find meme subreddits from which we can pull images in bulk.  \n",
    "According to this subreddit list I've pulled online (https://frontpagemetrics.com/list-all-subreddits),  \n",
    "there are 3 million subreddits, so we need to filter them down quite a bit!!  \n",
    "  \n",
    "#### Index Parameters\n",
    "* Subreddit must have 10,000+ subscribers\n",
    "* Subreddit must contain the word \"meme\" in its name or description* \n",
    "* No subreddits about fandoms, YouTube/IG/TikTok personalities, specific professions or hobbies.    \n",
    "\n",
    "That leaves us with political subreddits, subreddits devoted to specific memes, country/sexuality/religion/region-based meme/shitposting/circlejerk subreddits."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "#Creating a list of subreddits to sample from\n",
    "subreddits = pd.read_csv('subreddits-2021-07-23.csv')\n",
    "# we'll filter based on: \"meme\" in description/title + min. 10k subs\n",
    "# then manually filter to remove pages about pop culture/intellectual products, hobbies, etc, so it's only dedicated meme pages + political pages\n",
    "meme_subs = subreddits[\n",
    "    (subreddits['subs'] > 10000) &\n",
    "    (\n",
    "        subreddits['desc'].str.contains('meme') |\n",
    "        subreddits['real_name'].str.contains('meme')\n",
    "    )\n",
    "]\n",
    "#save the filtered dataset here, then review it\n",
    "meme_subs.to_csv('meme-subreddits-2021.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Scraping images from subreddits"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# initialize our reddit client\n",
    "reddit = praw.Reddit(\n",
    "    client_id='hS4CPqNExizMF7XJ1XlMBQ',\n",
    "    client_secret='zMcrLvT31UBI6V-zl2IFC9uR3MRA-g',\n",
    "    user_agent='reddit-meme-analysis 0.1.1 by /u/inkoh',\n",
    "    username='inkoh',\n",
    "    password='Songoku777'\n",
    ")\n",
    "reddit.read_only = True"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "#now, collect images\n",
    "subreddit_index = pd.read_csv('subreddit-index.csv')\n",
    "subreddits = subreddit_index[\"real_name\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# mass post scraping method\n",
    "def scrape_subreddits(subreddits):\n",
    "    posts = []\n",
    "    i = 0\n",
    "    for sub in subreddits:\n",
    "        sub_posts = scrape_posts_from_subreddit(sub)\n",
    "        for post in sub_posts:\n",
    "            posts.append(post)\n",
    "        pd.DataFrame(posts).to_csv('scraped-posts-bulk.csv')\n",
    "        time.sleep(15)\n",
    "    return posts\n",
    "\n",
    "\n",
    "def scrape_posts_from_subreddit(sub_name):\n",
    "    posts = []\n",
    "    try:\n",
    "        query = reddit.subreddit(sub_name).top(time_filter=\"year\", limit=None)\n",
    "        for submission in query:\n",
    "            submission_is_image = re.search(\n",
    "                '([^\\\\s]+(\\\\.(?i)(jpe?g|png|gif|bmp|webp))$)', submission.url)\n",
    "            if submission_is_image:\n",
    "                post_data = {\n",
    "                    \"reddit_id\": submission.id,\n",
    "                    \"image_url\": submission.url,\n",
    "                    \"title\": submission.title,\n",
    "                    \"author\": submission.author,\n",
    "                    \"subreddit\": sub_name,\n",
    "                    \"permalink\": \"https://reddit.com/r/\" + sub_name + \"/comments/\" + submission.id,\n",
    "                    \"num_comments\": submission.num_comments,\n",
    "                    \"num_upvotes\": submission.score,\n",
    "                    \"upvote_ratio\": submission.upvote_ratio\n",
    "                }\n",
    "                posts.append(post_data)\n",
    "    except:\n",
    "        print(\"an error occured :(\")\n",
    "    finally:\n",
    "        return posts\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# execute mass scrape\n",
    "# WARNING: takes a while (approx. 2.5hrs) to finish running\n",
    "posts = scrape_subreddits(subreddits)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "raw_posts = pd.read_csv('scraped-posts-bulk.csv')\n",
    "shuffled_posts = raw_posts.sample(frac=1)\n",
    "shuffled_posts.to_csv('raw-posts.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "interpreter": {
   "hash": "dca0ade3e726a953b501b15e8e990130d2b7799f14cfd9f4271676035ebe5511"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}